{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11021532,"sourceType":"datasetVersion","datasetId":6622431}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:26:27.030403Z","iopub.execute_input":"2025-03-23T20:26:27.030826Z","iopub.status.idle":"2025-03-23T20:26:27.039390Z","shell.execute_reply.started":"2025-03-23T20:26:27.030773Z","shell.execute_reply":"2025-03-23T20:26:27.037647Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport calendar\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.stattools import acf, pacf\nfrom gensim.models import Word2Vec\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:26:27.040721Z","iopub.execute_input":"2025-03-23T20:26:27.041044Z","iopub.status.idle":"2025-03-23T20:26:27.065862Z","shell.execute_reply.started":"2025-03-23T20:26:27.041016Z","shell.execute_reply":"2025-03-23T20:26:27.064653Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_csv('../input/9309a1dataset/recipe_data.csv')\n# frequent for every feature\ntop_5_preparation = df.groupby('User_ID')['Preparation_Steps'].value_counts().groupby(level=0, group_keys=False).head(5)\ntop_5_ingredient = df.groupby('User_ID')['Ingredients_List'].value_counts().groupby(level=0, group_keys=False).head(5)\ntop_5_type = df.groupby('User_ID')['Cuisine_Type'].value_counts().groupby(level=0, group_keys=False).head(5)\n\n# most frequent\ntop_1_preparation = df.groupby('User_ID')['Preparation_Steps'].value_counts().groupby(level=0, group_keys=False).head(1).reset_index(name='count')\ntop_1_ingredient = df.groupby('User_ID')['Ingredients_List'].value_counts().groupby(level=0, group_keys=False).head(1).reset_index(name='count')\ntop_1_type = df.groupby('User_ID')['Cuisine_Type'].value_counts().groupby(level=0, group_keys=False).head(1).reset_index(name='count')\n\ntop_1_preparation = top_1_preparation.rename(columns={'Preparation_Steps': 'Top_Pre'})\ntop_1_ingredient = top_1_ingredient.rename(columns={'Ingredients_List': 'Top_Ing'})\ntop_1_type = top_1_type.rename(columns={'Cuisine_Type': 'Top_Type'})\n\ntop_5_type_df = top_5_type.reset_index(name='Type_Count')\ntop_5_preparation_df = top_5_preparation.reset_index(name='Pre_Count')\ntop_5_ingredient_df = top_5_ingredient.reset_index(name='Ing_Count')\n\n\nuser_total = df.groupby('User_ID').size()\nuser_total_df = user_total.reset_index(name='Total_Time')\n\ndf = pd.merge(df, top_5_type_df[['User_ID', 'Cuisine_Type', 'Type_Count']], on=['User_ID', 'Cuisine_Type'], how='left')\ndf = pd.merge(df, top_5_preparation_df[['User_ID', 'Preparation_Steps', 'Pre_Count']], on=['User_ID', 'Preparation_Steps'], how='left')\ndf = pd.merge(df, top_5_ingredient_df[['User_ID', 'Ingredients_List', 'Ing_Count']], on=['User_ID', 'Ingredients_List'], how='left')\n\ndf = pd.merge(df, top_1_type[['User_ID', 'Top_Type']], on=['User_ID'], how='left')\ndf = pd.merge(df, top_1_preparation[['User_ID', 'Top_Pre']], on=['User_ID'], how='left')\ndf = pd.merge(df, top_1_ingredient[['User_ID', 'Top_Ing']], on=['User_ID'], how='left')\n\ndf = pd.merge(df, user_total_df[['User_ID', 'Total_Time']], on=['User_ID'], how='left')\n\nprint(df.describe())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:26:27.068436Z","iopub.execute_input":"2025-03-23T20:26:27.068788Z","iopub.status.idle":"2025-03-23T20:26:27.218407Z","shell.execute_reply.started":"2025-03-23T20:26:27.068756Z","shell.execute_reply":"2025-03-23T20:26:27.217158Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def cal_ing_freq(row):\n    return row['Ing_Count'] *1.0 / row['Total_Time']\n\ndef cal_pre_freq(row):\n    return row['Pre_Count'] *1.0 / row['Total_Time']\n\ndef cal_type_freq(row):\n    return row['Type_Count'] *1.0 / row['Total_Time']\n\n# Apply the function to each row\ndf['Type_Freq'] = df.apply(cal_type_freq, axis=1)\ndf['Pre_Freq'] = df.apply(cal_pre_freq, axis=1)\ndf['Ing_Freq'] = df.apply(cal_ing_freq, axis=1)\n\nprint(df.head(1))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:26:27.219985Z","iopub.execute_input":"2025-03-23T20:26:27.220336Z","iopub.status.idle":"2025-03-23T20:26:27.371602Z","shell.execute_reply.started":"2025-03-23T20:26:27.220300Z","shell.execute_reply":"2025-03-23T20:26:27.370239Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"stop_words = set(stopwords.words('english'))\n\ndef preprocess(text):\n    tokens = text.lower().replace(',', '').split()\n    return [word for word in tokens if word not in stop_words]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:26:27.372697Z","iopub.execute_input":"2025-03-23T20:26:27.372991Z","iopub.status.idle":"2025-03-23T20:26:27.379248Z","shell.execute_reply.started":"2025-03-23T20:26:27.372966Z","shell.execute_reply":"2025-03-23T20:26:27.378149Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ing_sentences = [preprocess(sentence) for sentence in df['Ingredients_List'].tolist() + df['Top_Ing'].tolist()]\ning_model = Word2Vec(ing_sentences, vector_size=100, window=5, min_count=1, workers=4)\n\ndef get_sentence_vector(sentence, model):\n    words = preprocess(sentence)\n    word_vectors = [model.wv[word] for word in words if word in model.wv]\n    if len(word_vectors) == 0:\n        return np.zeros(model.vector_size)\n    return np.mean(word_vectors, axis=0)\n\ndef calculate_similarity(row, model, c1, c2):\n    vector1 = get_sentence_vector(row[c1], model)\n    vector2 = get_sentence_vector(row[c2], model)\n    similarity = cosine_similarity([vector1], [vector2])\n    return similarity[0][0]\n\n# Apply the function to calculate similarity for each row\ndf['Ing_Sim'] = df.apply(calculate_similarity, axis=1, model=ing_model,c1='Ingredients_List', c2='Top_Ing')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:26:27.380501Z","iopub.execute_input":"2025-03-23T20:26:27.380806Z","iopub.status.idle":"2025-03-23T20:26:29.668307Z","shell.execute_reply.started":"2025-03-23T20:26:27.380777Z","shell.execute_reply":"2025-03-23T20:26:29.667260Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pre_sentences = [preprocess(sentence) for sentence in df['Preparation_Steps'].tolist() + df['Top_Pre'].tolist()]\npre_model = Word2Vec(pre_sentences, vector_size=100, window=5, min_count=1, workers=4)\n\ndef get_sentence_vector(sentence, model):\n    words = preprocess(sentence)\n    word_vectors = [model.wv[word] for word in words if word in model.wv]\n    if len(word_vectors) == 0:\n        return np.zeros(model.vector_size)\n    return np.mean(word_vectors, axis=0)\n\n# Apply the function to calculate similarity for each row\ndf['Pre_Sim'] = df.apply(calculate_similarity, axis=1, model=pre_model, c1='Preparation_Steps', c2='Top_Pre')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:26:29.669424Z","iopub.execute_input":"2025-03-23T20:26:29.669704Z","iopub.status.idle":"2025-03-23T20:26:31.911564Z","shell.execute_reply.started":"2025-03-23T20:26:29.669679Z","shell.execute_reply":"2025-03-23T20:26:31.910458Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(df.columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:26:31.912747Z","iopub.execute_input":"2025-03-23T20:26:31.913156Z","iopub.status.idle":"2025-03-23T20:26:31.919330Z","shell.execute_reply.started":"2025-03-23T20:26:31.913112Z","shell.execute_reply":"2025-03-23T20:26:31.918083Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# check Allergen_Information\n\ndef check_aller(row):\n    index = str(row['Ingredients_List']).find(str(row['Allergen_Information']))\n    if index == -1:\n        return 0\n    return 1\ndf['Is_Aller'] = df.apply(check_aller, axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:26:31.922587Z","iopub.execute_input":"2025-03-23T20:26:31.922918Z","iopub.status.idle":"2025-03-23T20:26:31.988484Z","shell.execute_reply.started":"2025-03-23T20:26:31.922889Z","shell.execute_reply":"2025-03-23T20:26:31.987377Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(df.head(10))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:26:31.990268Z","iopub.execute_input":"2025-03-23T20:26:31.990817Z","iopub.status.idle":"2025-03-23T20:26:32.007280Z","shell.execute_reply.started":"2025-03-23T20:26:31.990782Z","shell.execute_reply":"2025-03-23T20:26:32.006078Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# df.to_csv('/kaggle/working/recipe_processed.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:26:32.008785Z","iopub.execute_input":"2025-03-23T20:26:32.009231Z","iopub.status.idle":"2025-03-23T20:26:32.018156Z","shell.execute_reply.started":"2025-03-23T20:26:32.009183Z","shell.execute_reply":"2025-03-23T20:26:32.016710Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['Cooking_Time_Minutes'].hist(bins=100, figsize=(10, 6))\nplt.xlabel('Minutes ') \nplt.ylabel('Density') \nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:26:32.019455Z","iopub.execute_input":"2025-03-23T20:26:32.019878Z","iopub.status.idle":"2025-03-23T20:26:32.336123Z","shell.execute_reply.started":"2025-03-23T20:26:32.019836Z","shell.execute_reply":"2025-03-23T20:26:32.334846Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['User_Rating'].hist(bins=12, figsize=(10, 6))\nplt.xlabel('Rating') \nplt.ylabel('Density') \nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:26:32.336954Z","iopub.execute_input":"2025-03-23T20:26:32.337262Z","iopub.status.idle":"2025-03-23T20:26:32.544114Z","shell.execute_reply.started":"2025-03-23T20:26:32.337235Z","shell.execute_reply":"2025-03-23T20:26:32.542921Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":" \ndf['Serving_Size'].hist(bins=12, figsize=(10, 6))\nplt.xlabel('People Amount') \nplt.ylabel('Density') \nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:26:32.545111Z","iopub.execute_input":"2025-03-23T20:26:32.545406Z","iopub.status.idle":"2025-03-23T20:26:32.723975Z","shell.execute_reply.started":"2025-03-23T20:26:32.545381Z","shell.execute_reply":"2025-03-23T20:26:32.722983Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":" \n\ndf['Calories_Per_Serving'].hist(bins=12, figsize=(10, 6))\nplt.xlabel('Calory') \nplt.ylabel('Density') \nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:26:32.724902Z","iopub.execute_input":"2025-03-23T20:26:32.725258Z","iopub.status.idle":"2025-03-23T20:26:32.907013Z","shell.execute_reply.started":"2025-03-23T20:26:32.725224Z","shell.execute_reply":"2025-03-23T20:26:32.905840Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['Cost_Per_Serving'].hist(bins=30, figsize=(10, 6))\nplt.xlabel('$') \nplt.ylabel('Density') \nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:26:32.908042Z","iopub.execute_input":"2025-03-23T20:26:32.908452Z","iopub.status.idle":"2025-03-23T20:26:33.145364Z","shell.execute_reply.started":"2025-03-23T20:26:32.908415Z","shell.execute_reply":"2025-03-23T20:26:33.144260Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['Popularity_Score'].hist(bins=500, figsize=(10, 6))\nplt.xlabel('Popularity') \nplt.ylabel('Density') \nplt.show()\n ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:26:33.146473Z","iopub.execute_input":"2025-03-23T20:26:33.146885Z","iopub.status.idle":"2025-03-23T20:26:34.062446Z","shell.execute_reply.started":"2025-03-23T20:26:33.146846Z","shell.execute_reply":"2025-03-23T20:26:34.061148Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['Seasonal_Availability'].hist(bins=3, figsize=(10, 6))\nplt.xlabel('Type') \nplt.ylabel('Density') \nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:26:34.063583Z","iopub.execute_input":"2025-03-23T20:26:34.064021Z","iopub.status.idle":"2025-03-23T20:26:34.219056Z","shell.execute_reply.started":"2025-03-23T20:26:34.063976Z","shell.execute_reply":"2025-03-23T20:26:34.217689Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import math\n\ndef wilson_score(row):\n    \n    z = 1.96\n    x = row['User_Rating']\n    n = row['Popularity_Score']\n    if n == 0:\n        return 0.0  # 如果没有评分，返回 0 的估计值\n    p_hat = x / n\n    denominator = 1 + (z**2 / n)\n    center_adjusted_probability = p_hat + (z**2 / (2 * n))\n    adjusted_stddev = z * math.sqrt((p_hat * (1 - p_hat) / n) + (z**2 / (4 * n**2)))\n    \n    lower_bound = (center_adjusted_probability - adjusted_stddev) / denominator\n    upper_bound = (center_adjusted_probability + adjusted_stddev) / denominator\n    return (lower_bound + upper_bound) / 2\n\ndf['wilson_score'] = df.apply(wilson_score, axis=1)\nprint(df.head(10))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:26:34.220217Z","iopub.execute_input":"2025-03-23T20:26:34.220526Z","iopub.status.idle":"2025-03-23T20:26:34.296333Z","shell.execute_reply.started":"2025-03-23T20:26:34.220496Z","shell.execute_reply":"2025-03-23T20:26:34.295094Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\n\ndef mean_absolute_error(y_true, y_pred):\n    return sum(abs(y - y_hat) for y, y_hat in zip(y_true, y_pred)) / len(y_true)\n\n\ndef getResult(x_train, y_train, x_test, y_test, model):\n    model.fit(x_train, y_train)\n    y_pred = model.predict(x_test)\n    mse = mean_squared_error(y_test, y_pred)\n    rmse = np.sqrt(mse)\n    mae = mean_absolute_error(y_pred, y_test)\n    print('mse: ', mse)\n    print('rmse: ', rmse)\n    print('mae: ', mae)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:29:19.440849Z","iopub.execute_input":"2025-03-23T20:29:19.441305Z","iopub.status.idle":"2025-03-23T20:29:19.449110Z","shell.execute_reply.started":"2025-03-23T20:29:19.441269Z","shell.execute_reply":"2025-03-23T20:29:19.447933Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(df.columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:26:34.305724Z","iopub.execute_input":"2025-03-23T20:26:34.306055Z","iopub.status.idle":"2025-03-23T20:26:34.324262Z","shell.execute_reply.started":"2025-03-23T20:26:34.306021Z","shell.execute_reply":"2025-03-23T20:26:34.322947Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df1 = df[['Cooking_Time_Minutes', 'User_Rating', 'Serving_Size', 'Calories_Per_Serving', 'Cost_Per_Serving', 'Popularity_Score', 'Cuisine_Type', 'Difficulty_Level', 'Occasion', 'Seasonal_Availability', 'Type_Freq', 'Pre_Freq', 'Ing_Freq']]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:26:34.325485Z","iopub.execute_input":"2025-03-23T20:26:34.325917Z","iopub.status.idle":"2025-03-23T20:26:34.344738Z","shell.execute_reply.started":"2025-03-23T20:26:34.325870Z","shell.execute_reply":"2025-03-23T20:26:34.343178Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset2_dummy = pd.get_dummies(df1, columns=['Cuisine_Type', 'Difficulty_Level', 'Occasion', 'Seasonal_Availability'])\ntarget = dataset2_dummy['User_Rating']\nfeatures = dataset2_dummy.drop(columns=['User_Rating'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:26:34.346043Z","iopub.execute_input":"2025-03-23T20:26:34.346477Z","iopub.status.idle":"2025-03-23T20:26:34.377103Z","shell.execute_reply.started":"2025-03-23T20:26:34.346440Z","shell.execute_reply":"2025-03-23T20:26:34.375830Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.linear_model import Lasso\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import Ridge\nfrom sklearn.ensemble import GradientBoostingRegressor\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split\n\n\n\nrf_regressor = RandomForestRegressor(\n    n_estimators=100,          # Number of trees\n    max_depth=None,             # No limit on the depth of trees\n    min_samples_split=2,        # Minimum samples required to split an internal node\n    min_samples_leaf=1,         # Minimum samples required to be at a leaf node\n    max_features='auto',        # Consider all features when splitting nodes\n    bootstrap=True,             # Use bootstrap sampling\n    random_state=42            # Ensure reproducibility\n)\n\n\ngbdt = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\nridge_regressor = Ridge(alpha=1.0)\nlasso = Lasso(alpha=1.0) \nxgb_regressor = xgb.XGBRegressor(\n    n_estimators=100,          # Number of trees\n    learning_rate=0.1,         # Learning rate\n    max_depth=5,               # Depth of each tree\n    min_child_weight=1,        # Minimum sum of instance weight needed in a child\n    subsample=0.8,             # Fraction of samples used for fitting trees\n    colsample_bytree=0.8,      # Fraction of features used for each tree\n    gamma=0,                   # Minimum loss reduction for pruning\n    reg_alpha=0,               # L1 regularization\n    reg_lambda=1,              # L2 regularization\n    objective='reg:squarederror',  # Standard regression objective\n    random_state=42\n)\n\nx_train, x_test, y_train, y_test = train_test_split(features, target, test_size=0.3, random_state=42)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:26:34.378664Z","iopub.execute_input":"2025-03-23T20:26:34.379045Z","iopub.status.idle":"2025-03-23T20:26:34.391758Z","shell.execute_reply.started":"2025-03-23T20:26:34.379012Z","shell.execute_reply":"2025-03-23T20:26:34.390375Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"getResult(x_train, y_train, x_test, y_test, rf_regressor)\ngetResult(x_train, y_train, x_test, y_test, gbdt)\ngetResult(x_train, y_train, x_test, y_test, ridge_regressor)\ngetResult(x_train, y_train, x_test, y_test, lasso)\ngetResult(x_train, y_train, x_test, y_test, xgb_regressor)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:29:23.710051Z","iopub.execute_input":"2025-03-23T20:29:23.710448Z","iopub.status.idle":"2025-03-23T20:29:27.179126Z","shell.execute_reply.started":"2025-03-23T20:29:23.710415Z","shell.execute_reply":"2025-03-23T20:29:27.177875Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\n\n\ndnn = Sequential()\ndnn.add(Dense(64, input_dim=x_train.shape[1], activation='relu'))\ndnn.add(Dense(64, activation='relu'))\ndnn.add(Dense(32, activation='relu'))\ndnn.add(Dense(1))\n\ndnn.compile(optimizer='adam', loss='mean_squared_error')\ngetResult(x_train, y_train, x_test, y_test, dnn)\n\ndef getDNNResult(x_train, y_train, x_test, y_test):\n    dnn.fit(x_train, y_train, epochs=50, batch_size=32, validation_split=0.2)\n    y_pred = model.predict(x_test)\n    mse = mean_squared_error(y_test, y_pred)\n    rmse = np.sqrt(mse)\n    mae = mean_absolute_error(y_pred, y_test)\n    print('mse: ', mse)\n    print('rmse: ', rmse)\n    print('mae: ', mae)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:44:20.224692Z","iopub.execute_input":"2025-03-23T20:44:20.225043Z","iopub.status.idle":"2025-03-23T20:44:22.847293Z","shell.execute_reply.started":"2025-03-23T20:44:20.225014Z","shell.execute_reply":"2025-03-23T20:44:22.845929Z"}},"outputs":[],"execution_count":null}]}